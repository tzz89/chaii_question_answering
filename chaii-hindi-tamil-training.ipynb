{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Learning references\n1. BPE for Tamil/Hindi \\\n   a. https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=BPE#models\n2. Extractive Question and Answering Task\\\n    a. Main reference https://github.com/huggingface/notebooks/blob/master/examples/question_answering.ipynb \\\n    b. How to handle long context paragraphs that are longer than pretrained max_length\n3. Extractive Q&A architecture\n    a. https://www.youtube.com/watch?v=l8ZYCvgGu0o","metadata":{}},{"cell_type":"markdown","source":"# Trials\n1. Training with provided dataset only\n2. Training with external dataset + provided dataset","metadata":{}},{"cell_type":"code","source":"!pip install openpyxl\n# For training in TPU\n# !pip install accelerate\n# !pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.7-cp37-cp37m-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:56:42.7145Z","iopub.execute_input":"2021-10-18T03:56:42.714805Z","iopub.status.idle":"2021-10-18T03:56:52.272612Z","shell.execute_reply.started":"2021-10-18T03:56:42.71473Z","shell.execute_reply":"2021-10-18T03:56:52.271733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#torch libraries\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n#tensorflow libraries\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n#huggingface libraries\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModel,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup\n)\n# For training with TPU\n# from accelerate import Accelerator\n# from accelerate import notebook_launcher\n\n\n#sklearn libraries\nfrom sklearn.model_selection import StratifiedKFold\n\n# python libraries\nimport numpy as np \nimport multiprocessing\nimport pandas as pd \nimport random\nimport os\nimport gc\nimport time\nfrom tqdm.notebook import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T03:56:52.275683Z","iopub.execute_input":"2021-10-18T03:56:52.27624Z","iopub.status.idle":"2021-10-18T03:57:02.599268Z","shell.execute_reply.started":"2021-10-18T03:56:52.276207Z","shell.execute_reply":"2021-10-18T03:57:02.59849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CONFIG:\n    #Debug\n    debug = False\n    debug_sample = 200 #use a smaller dataset for debug\n    perform_fold = [4]\n    \n    #misc\n    seed = 42\n    num_workers = 2\n    \n    #training params \n    train_batchsize = 4\n    val_batchsize = 8\n    epochs = 2\n    n_splits = 5\n    \n    #model params\n    model = 'deepset/xlm-roberta-large-squad2'\n    max_input_length = 384 #Hyperparameter to be tuned, following the guide from huggingface\n    doc_stride = 128  #Hyperparameter to be tuned, following the guide from huggingface\n    \n    \n    #optimizer\n    optimizer = \"AdamW\" #implemented AdamW, Adam, SGD\n    \n    max_grad_norm = 1.0 #gradient clipping to prevent exploding gradient\n    \n    if (optimizer == \"AdamW\") or (optimizer == \"Adam\"): \n        optimizer_params = dict(\n            betas = (0.9,0.999), \n            lr = 0.00001,\n            eps = 1e-8,\n            weight_decay= 0.01,\n            amsgrad = False\n        )\n        \n    elif optimizer == \"SGD\":\n        optimizer_params = dict( \n            lr = 0.001,\n            momentum = 0,\n            weight_decay =0,\n            dampening = 0,\n            nesterov = False\n        )\n    \n\n    \n    #scheduler\n    #implemented  pytorch CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateu, \n    scheduler = \"cosine_with_warmup\"\n    if scheduler == \"CosineAnnealing\":\n        scheduler_params = dict(\n            T_max = 3,\n            eta_min = 0,\n            last_epoch = -1,\n            verbose=True\n        )\n    elif scheduler == \"ReduceLROnPlateu\":\n        scheduler_params = dict(\n            mode = 'min',\n            factor = 0.1,\n            patience = 10,\n            threshold = 1e-4,\n            threshold_mode= 'rel',\n            cooldown = 0,\n            min_lr = 0,\n            eps = 1e-8,\n            verbose=True\n        )\n    elif scheduler == \"CosineAnnealingWarmRestarts\":\n        scheduler_params = dict(\n            T_0  = 3,\n            T_mult  = 1,\n            eta_min = 0,\n            last_epoch = -1,\n            verbose = True\n        )\n        \n    elif scheduler == \"linear_with_warmup\": #huggingface scheduler\n        scheduler_params = dict(\n            warmup_steps_ratio =  0.1\n        )\n        \n    elif scheduler == \"cosine_with_warmup\": #huggingface scheduler\n        scheduler_params = dict(\n            warmup_steps_ratio =  0.1\n        )\n        \n    \n    #SWA stochastic weight averaging\n    SWA = False\n     \n    #FP16\n    FP16 = False #torch.cuda.amp does not seem to work in this case as the loss did not decrease\n    \n    #accelerate \n    ACCELERATE = False\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:02.600708Z","iopub.execute_input":"2021-10-18T03:57:02.600967Z","iopub.status.idle":"2021-10-18T03:57:02.613606Z","shell.execute_reply.started":"2021-10-18T03:57:02.600918Z","shell.execute_reply":"2021-10-18T03:57:02.612832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting Seed","metadata":{}},{"cell_type":"code","source":"def set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \n\ndef set_torch_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n#     torch.backends.cudnn.benchmark = False \n#     torch.backends.cudnn.deterministic =True\n\ndef set_tf_seed(seed):\n    tf.random.set_seed(seed)  \n    \nset_random_seed(CONFIG.seed)\nset_torch_seed(CONFIG.seed)\n# set_tf_seed(CONFIG.seed)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:02.615925Z","iopub.execute_input":"2021-10-18T03:57:02.616327Z","iopub.status.idle":"2021-10-18T03:57:02.630645Z","shell.execute_reply.started":"2021-10-18T03:57:02.616271Z","shell.execute_reply":"2021-10-18T03:57:02.6299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading the data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"../input/chaii-hindi-and-tamil-question-answering/train.csv\")\n\nif CONFIG.debug:\n    train_df = train_df.sample(CONFIG.debug_sample)\n\ntrain_df['fold'] = -1\nk_fold = StratifiedKFold(n_splits=CONFIG.n_splits, shuffle=True, random_state=CONFIG.seed)\nfor fold_num , (train_idx, test_idx) in enumerate(k_fold.split(train_df['id'], y=train_df['language'])):\n    train_df.iloc[test_idx,-1]= fold_num\n\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:02.632369Z","iopub.execute_input":"2021-10-18T03:57:02.632708Z","iopub.status.idle":"2021-10-18T03:57:03.74645Z","shell.execute_reply.started":"2021-10-18T03:57:02.632659Z","shell.execute_reply":"2021-10-18T03:57:03.745784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Adding external dataset","metadata":{}},{"cell_type":"code","source":"external_mlqa = pd.read_csv('../input/mlqa-hindi-processed/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('../input/mlqa-hindi-processed/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])\nexternal_train['id'] = list(np.arange(1, len(external_train)+1))\nexternal_train['fold'] = -1\ntrain_df = pd.concat([train_df, external_train]).reset_index(drop=True)\ntrain_df","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:03.747818Z","iopub.execute_input":"2021-10-18T03:57:03.748088Z","iopub.status.idle":"2021-10-18T03:57:04.260057Z","shell.execute_reply.started":"2021-10-18T03:57:03.748055Z","shell.execute_reply":"2021-10-18T03:57:04.259271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## While we cannot preprocess the context as it will affect the answer_start, we can clean up the question","metadata":{}},{"cell_type":"code","source":"def preprocess_question(df):\n    df['question'] = df['question'].str.strip()\n    return df\n\ntrain_df = preprocess_question(train_df)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:04.261372Z","iopub.execute_input":"2021-10-18T03:57:04.261626Z","iopub.status.idle":"2021-10-18T03:57:04.281642Z","shell.execute_reply.started":"2021-10-18T03:57:04.261592Z","shell.execute_reply":"2021-10-18T03:57:04.280565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Break up large context","metadata":{}},{"cell_type":"code","source":"def break_long_context(df, tokenizer, train=True):\n    if train: \n        n_examples = len(df)\n        full_train_set = []\n        for i in tqdm(range(n_examples)):\n            row = df.iloc[i]\n            # tokenizer parameters can be found here \n            # https://huggingface.co/transformers/internal/tokenization_utils.html#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n            tokenized_examples = tokenizer(row['question'],\n                                          row['context'],\n                                          padding='max_length',\n                                          max_length=CONFIG.max_input_length, \n                                          truncation='only_second',\n                                          stride=CONFIG.doc_stride,\n                                          return_overflowing_tokens=True, #returns the number of over flow\n                                          return_offsets_mapping=True     #returns the BPE mapping to the original word\n                                          ) \n            \n            # tokenized_example keys\n            #'input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping'\n            sample_mappings = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n            offset_mappings = tokenized_examples.pop(\"offset_mapping\")\n            \n            final_examples = [] #'input_ids','attention_mask' ,'start_position', 'end_position'\n            n_sub_examples = len(sample_mappings)\n            for j in range(n_sub_examples):\n                input_ids = tokenized_examples[\"input_ids\"][j]\n                attention_mask = tokenized_examples[\"attention_mask\"][j]\n                \n                sliced_text = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n                final_example = dict(input_ids = input_ids, \n                                     attention_mask = attention_mask,\n                                     sliced_text = sliced_text,\n                                     offset_mapping=offset_mappings[j],\n                                     fold=row['fold'])\n                \n                \n                \n                # Most of the time cls_index is 0\n                cls_index = input_ids.index(tokenizer.cls_token_id)\n                # None, 0, 0, .... None, None, 1, 1,.....\n                sequence_ids = tokenized_examples.sequence_ids(j)\n                \n                sample_index = sample_mappings[j]\n                offset_map = offset_mappings[j]\n                \n                if np.isnan(row[\"answer_start\"]) : # if no answer, start and end position is cls_index\n                    final_example['start_position'] = cls_index\n                    final_example['end_position'] = cls_index\n                    final_example['tokenized_answer'] = \"\"\n                    final_example['answer_text'] = \"\"\n                else:\n                    start_char  = row[\"answer_start\"]\n                    end_char  = start_char + len(row[\"answer_text\"])\n                    \n                    token_start_index = sequence_ids.index(1)\n                    token_end_index = len(sequence_ids)- 1 - (sequence_ids[::-1].index(1))\n                    \n                    if not (offset_map[token_start_index][0]<=start_char and offset_map[token_end_index][1] >= end_char):\n                        final_example['start_position'] = cls_index\n                        final_example['end_position'] = cls_index\n                        final_example['tokenized_answer'] = \"\"\n                        final_example['answer_text'] = \"\"\n                    else:\n                        #Move token_start_index to the correct context index\n                        while token_start_index < len(offset_map) and offset_map[token_start_index][0] <= start_char:\n                            token_start_index +=1\n                        final_example['start_position'] = token_start_index -1\n                        \n                        while offset_map[token_end_index][1] >= end_char: #Take note that we will want the end_index inclusively, we will need to slice properly later\n                            token_end_index -=1\n                        final_example['end_position'] = token_end_index + 1   \n                        tokenized_answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[final_example['start_position']:final_example['end_position']+1]))\n                        final_example['tokenized_answer'] = tokenized_answer\n                        final_example['answer_text'] = row['answer_text']\n                        \n                final_examples.append(final_example)\n            full_train_set += final_examples\n    \n            \n            \n        return full_train_set","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:04.283569Z","iopub.execute_input":"2021-10-18T03:57:04.284111Z","iopub.status.idle":"2021-10-18T03:57:04.302608Z","shell.execute_reply.started":"2021-10-18T03:57:04.284074Z","shell.execute_reply":"2021-10-18T03:57:04.301798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exporting to do a sanity check to make sure preprocessing is correct","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CONFIG.model)\n\nfull_train_set = break_long_context(train_df,tokenizer)\nfull_train_df = pd.DataFrame.from_dict(full_train_set)\nfull_train_df.to_excel(\"full_train_df.xlsx\")\n\nprint(f\"Total training examples = {len(full_train_set)}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:57:04.303683Z","iopub.execute_input":"2021-10-18T03:57:04.304106Z","iopub.status.idle":"2021-10-18T03:58:53.346168Z","shell.execute_reply.started":"2021-10-18T03:57:04.304068Z","shell.execute_reply":"2021-10-18T03:58:53.344518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"full_train_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.348855Z","iopub.execute_input":"2021-10-18T03:58:53.349137Z","iopub.status.idle":"2021-10-18T03:58:53.507351Z","shell.execute_reply.started":"2021-10-18T03:58:53.349105Z","shell.execute_reply":"2021-10-18T03:58:53.506531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base on anaylsis, we can do the follow post processing \npost processing \\\nstrip \\\n\\\"-\" #strip front maybe dangerous\\\n(\\\n)\\\n[\\\n]\n\nchange\\\nகிமீ2 -> கிமீ²\\ #kmsquare\nகி.மீ2 ->கி.மீ²\\ #kmsquare\nஇந்தியாவில் ->இந்தியா # from in india to india\\\nஇந்தியாவின்->இந்தியா # from india's to india\\\nஹென்றி பெக்கொரெலுக்கு ->ஹென்றி பெக்கொரெலு \\\nright strip என்றும் # always known as\\\nदोनों->दो #both to two","metadata":{}},{"cell_type":"markdown","source":"## Creating the dataset","metadata":{}},{"cell_type":"code","source":"full_train_set[0].keys()\n# we will only need input_ids, attention_mask, start_position and end_position for training","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.508703Z","iopub.execute_input":"2021-10-18T03:58:53.508974Z","iopub.status.idle":"2021-10-18T03:58:53.514789Z","shell.execute_reply.started":"2021-10-18T03:58:53.508927Z","shell.execute_reply":"2021-10-18T03:58:53.513871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ChaiDataset(Dataset):\n    def __init__(self, dataset, is_train=True):\n        super(ChaiDataset, self).__init__()\n        self.dataset = dataset #list of features\n        self.is_train= is_train\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index):\n        features = self.dataset[index]\n        if self.is_train:\n            return {\n                'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(features['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(features['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(features['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(features['input_ids'], dtype=torch.long),\n                'attention_mask': torch.tensor(features['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(features['offset_mapping'], dtype=torch.long),\n                'sequence_ids':features['sequence_ids'],\n                'id':features['example_id'],\n                'context':features['context'],\n                'question':features['question']\n            }\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.516264Z","iopub.execute_input":"2021-10-18T03:58:53.516567Z","iopub.status.idle":"2021-10-18T03:58:53.529514Z","shell.execute_reply.started":"2021-10-18T03:58:53.516522Z","shell.execute_reply":"2021-10-18T03:58:53.528702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating model class","metadata":{}},{"cell_type":"code","source":"class ChaiModel(nn.Module):\n    def __init__(self, model_config):\n        super(ChaiModel, self).__init__()\n        self.backbone = AutoModel.from_pretrained(CONFIG.model)\n        self.linear = nn.Linear(model_config.hidden_size, 2)\n        \n    def forward(self, input_ids, attention_mask):\n        model_output = self.backbone(input_ids, attention_mask=attention_mask)\n        sequence_output = model_output[0] # (batchsize, sequencelength, hidden_dim)\n        \n        qa_logits = self.linear(sequence_output) # (batchsize, sequencelength, 2)\n        start_logit, end_logit = qa_logits.split(1, dim=-1) #  (batchsize, sequencelength), 1), (batchsize, sequencelength, 1)\n        start_logits = start_logit.squeeze(-1) # remove last dim (batchsize, sequencelength)\n        end_logits = end_logit.squeeze(-1)    #remove last dim (batchsize, sequencelength)\n        \n        return start_logits, end_logits # (2,batchsize, sequencelength)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.53099Z","iopub.execute_input":"2021-10-18T03:58:53.531662Z","iopub.status.idle":"2021-10-18T03:58:53.540411Z","shell.execute_reply.started":"2021-10-18T03:58:53.531625Z","shell.execute_reply":"2021-10-18T03:58:53.539492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Utility functions","metadata":{}},{"cell_type":"code","source":"def loss_fn(y_pred, y_true):\n    start_position_preds, end_position_preds = y_pred # (batchsize, sequencelength),(batchsize, sequencelength)\n    start_position_trues, end_position_trues = y_true \n    \n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_position_preds,start_position_trues)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_position_preds,end_position_trues)\n    \n    return (start_loss+end_loss)/2\n\ndef get_optimizer(model):\n    if CONFIG.optimizer == \"AdamW\":\n        optimizer_params = {\"params\":model.parameters(), **CONFIG.optimizer_params}\n        optimizer = torch.optim.AdamW(**optimizer_params)\n    elif CONFIG.optimizer == \"Adam\":\n        optimizer_params = {\"params\":model.parameters(), **CONFIG.optimizer_params}\n        optimizer = torch.optim.Adam(**optimizer_params)\n    elif CONFIG.optimizer == \"SGD\":\n        optimizer_params = {\"params\":model.parameters(), **CONFIG.optimizer_params}\n        optimizer = torch.optim.SGD(**optimizer_params)\n    else:\n        raise NotImplementedError \n    \n    return optimizer\n\ndef get_scheduler(optimizer, total_steps_per_epoch):\n    if CONFIG.scheduler == \"CosineAnnealing\":\n        scheduler_params = CONFIG.scheduler_params\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, scheduler_params['T_max'], \n                                                               eta_min=scheduler_params['eta_min'],\n                                                               last_epoch=scheduler_params['last_epoch'],\n                                                               verbose=scheduler_params['verbose'])\n    elif CONFIG.scheduler == \"ReduceLROnPlateu\":\n        scheduler_params = {\"optimizer\":optimizer, **CONFIG.scheduler_params}      \n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(**scheduler_params)\n    elif CONFIG.scheduler == \"CosineAnnealingWarmRestarts\":\n        scheduler_params = {\"optimizer\":optimizer, **CONFIG.scheduler_params}   \n        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(**scheduler_params)\n    elif CONFIG.scheduler == \"linear_with_warmup\":\n        num_warmup_steps = int(CONFIG.scheduler_params[\"warmup_steps_ratio\"] * total_steps_per_epoch)\n        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps, total_steps_per_epoch)\n    elif CONFIG.scheduler == \"cosine_with_warmup\":\n        num_warmup_steps = int(CONFIG.scheduler_params[\"warmup_steps_ratio\"] * total_steps_per_epoch)\n        scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, total_steps_per_epoch)\n    else:\n        raise NotImplementedError \n        \n    return scheduler\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.542119Z","iopub.execute_input":"2021-10-18T03:58:53.5426Z","iopub.status.idle":"2021-10-18T03:58:53.557742Z","shell.execute_reply.started":"2021-10-18T03:58:53.542562Z","shell.execute_reply":"2021-10-18T03:58:53.557041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_data_loaders(train_data, val_data):\n    train_dataset = ChaiDataset(train_data)\n    val_dataset = ChaiDataset(val_data)\n    \n    train_dataloader = DataLoader(train_dataset, \n                                  CONFIG.train_batchsize, \n                                  shuffle=True, \n                                  num_workers= CONFIG.num_workers,\n                                  drop_last=False,\n                                  pin_memory=True)\n    \n    val_dataloader = DataLoader(val_dataset, \n                                CONFIG.val_batchsize, \n                                shuffle= False, \n                                num_workers= CONFIG.num_workers,\n                                drop_last=False,\n                                pin_memory=True)\n    \n    return train_dataloader, val_dataloader","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.559209Z","iopub.execute_input":"2021-10-18T03:58:53.559526Z","iopub.status.idle":"2021-10-18T03:58:53.569675Z","shell.execute_reply.started":"2021-10-18T03:58:53.55949Z","shell.execute_reply":"2021-10-18T03:58:53.56893Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.max = 0\n        self.min = 1e5\n    \n    def update(self, val, count=1):\n        self.sum += val * count\n        self.count += count\n        self.val = val\n        self.avg = self.sum/self.count\n        if self.max < val:\n            self.max = val\n        if self.min > val:\n            self.min = val\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.571006Z","iopub.execute_input":"2021-10-18T03:58:53.571345Z","iopub.status.idle":"2021-10-18T03:58:53.580827Z","shell.execute_reply.started":"2021-10-18T03:58:53.571312Z","shell.execute_reply":"2021-10-18T03:58:53.580014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(model, train_dataloader, optimizer,scheduler, device, scaler=None):\n    model.train() #switch to train mode\n    model.zero_grad()\n    \n    loss_meter = AverageMeter()\n    prog_bar = tqdm(train_dataloader, desc='Train')\n    for features in prog_bar:\n        #move data to device\n        input_ids = features['input_ids'].to(device)\n        attention_mask = features['attention_mask'].to(device)\n        start_position = features['start_position'].to(device)\n        end_position = features['end_position'].to(device)\n            \n        if scaler is None:\n            #forward\n            y_pred = model(input_ids=input_ids, attention_mask=attention_mask)\n            loss = loss_fn(y_pred, (start_position, end_position))\n            \n            #backward\n            optimizer.zero_grad() #clear any gradient first\n            loss.backward()\n\n            #update optimizer and zero gradient\n            optimizer.step()\n            scheduler.step()\n                   \n        else:\n            with torch.cuda.amp.autocast():\n                y_pred = model(input_ids=input_ids, attention_mask=attention_mask)\n                loss = loss_fn(y_pred, (start_position, end_position))\n                \n                #backward\n                optimizer.zero_grad() #clear any gradient first\n                scaler.scale(loss).backward()\n                scaler.step(optimizer)\n                scaler.update()\n                \n        loss_meter.update(loss.detach().item(), input_ids.shape[0])\n        prog_bar.set_postfix({\"train_loss\": loss_meter.avg})\n        \n    return loss_meter.avg\n        \n    \ndef evaluation_step(model, val_dataloader, device):\n    model.eval()\n    loss_meter = AverageMeter()\n    prog_bar = tqdm(val_dataloader)\n    full_pred = []\n    with torch.no_grad():\n        for features in prog_bar:\n            input_ids = features['input_ids'].to(device)\n            attention = features['attention_mask'].to(device)\n            start_position = features['start_position'].to(device)\n            end_position = features['end_position'].to(device)\n            \n            #forward\n            y_pred = model(input_ids, attention_mask=attention)\n            loss = loss_fn(y_pred, (start_position, end_position))\n            loss_meter.update(loss.detach().item(), input_ids.shape[0])\n            prog_bar.set_postfix({\"val_loss\":loss_meter.avg})\n        \n    return loss_meter.avg","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.583844Z","iopub.execute_input":"2021-10-18T03:58:53.584085Z","iopub.status.idle":"2021-10-18T03:58:53.599653Z","shell.execute_reply.started":"2021-10-18T03:58:53.584061Z","shell.execute_reply":"2021-10-18T03:58:53.598974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Main training loop","metadata":{}},{"cell_type":"markdown","source":"### Hardware selection","metadata":{}},{"cell_type":"code","source":"if CONFIG.ACCELERATE:\n    accelerator = Accelerator()\n    device = accelerator.device\nelse:\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    \nprint(f\"Using {device}\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.601485Z","iopub.execute_input":"2021-10-18T03:58:53.601936Z","iopub.status.idle":"2021-10-18T03:58:53.655845Z","shell.execute_reply.started":"2021-10-18T03:58:53.6019Z","shell.execute_reply":"2021-10-18T03:58:53.655082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for n_fold in range(CONFIG.n_splits):\n    if n_fold in CONFIG.perform_fold:\n        train_features = full_train_df[full_train_df['fold']!=n_fold].to_dict('records')\n        val_features   = full_train_df[full_train_df['fold']==n_fold].to_dict('records')\n\n        train_dataloader, val_dataloader = get_data_loaders(train_features, val_features)\n\n        total_steps = len(train_dataloader) * CONFIG.epochs\n\n        config = AutoConfig.from_pretrained(CONFIG.model)\n        model = ChaiModel(config)\n        model.to(device)\n        optimizer = get_optimizer(model)\n        scheduler = get_scheduler(optimizer,total_steps)\n\n        if CONFIG.FP16:\n            scaler = torch.cuda.amp.GradScaler()\n        else:\n            scaler = None\n\n\n        best_val_loss = 1e3\n\n        for epoch in range(CONFIG.epochs):\n            print(f\"EPOCH {epoch}\")\n\n            train_loss = train_step(model, train_dataloader, optimizer,scheduler, device, scaler)\n            val_loss = evaluation_step(model, val_dataloader, device)\n\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                torch.save(model.state_dict(), f\"./pytorch_model_fold_{n_fold}_epoch{epoch}.pth\")\n\n        del model\n        del optimizer\n        del scheduler\n        del train_dataloader\n        del val_dataloader\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T03:58:53.657837Z","iopub.execute_input":"2021-10-18T03:58:53.658476Z","iopub.status.idle":"2021-10-18T05:49:14.055644Z","shell.execute_reply.started":"2021-10-18T03:58:53.658434Z","shell.execute_reply":"2021-10-18T05:49:14.05477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"done\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T05:49:28.653028Z","iopub.execute_input":"2021-10-18T05:49:28.653311Z","iopub.status.idle":"2021-10-18T05:49:28.65815Z","shell.execute_reply.started":"2021-10-18T05:49:28.653281Z","shell.execute_reply":"2021-10-18T05:49:28.657296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}